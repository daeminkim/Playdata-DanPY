{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"name":"09_GAP_Transfer Learning(전이학습).ipynb","provenance":[],"collapsed_sections":["Y_EDnXTwNv8a"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"551.4px","left":"1166px","right":"20px","top":"120px","width":"350px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"dIwP9psBNv8M"},"source":["# GlobalAveragePooling (GAP)\n","- Feature map의 채널별로 평균값을 추출 1 x 1 x channel 의 Feature map을 생성\n","- `model.add(keras.layers.GlobalAveragePooling2D())`\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"nLfDW6L4Nv8T"},"source":["- Feature Extraction layer에서 추출한 Feature map을 Classifier layer로 Flatten해서 전달하면 많은 연결노드와 파라미터가 필요하게된다. GAP를 사용하면 노드와 파라미터의 개수를 효과적으로 줄일 수 있다.\n","- Feature map의 채널수가 많을 경우 GAP를 사용하는 것이 효과적이나 채널수가 적다면 Flatten을 사용하는 것이 좋다.\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"4qc1azjoNv8U","executionInfo":{"status":"ok","timestamp":1619615776314,"user_tz":-540,"elapsed":3911,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"4def1c00-2460-4204-e6ba-50c43c7412ba"},"source":["# 이미지 다운로드\n","import gdown\n","url = 'https://drive.google.com/uc?id=1nBE3N2cXQGwD8JaD0JZ2LmFD-n3D5hVU'\n","fname = 'cats_and_dogs_small.zip'\n","gdown.download(url, fname, quiet=False)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1nBE3N2cXQGwD8JaD0JZ2LmFD-n3D5hVU\n","To: /content/cats_and_dogs_small.zip\n","90.8MB [00:00, 234MB/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cats_and_dogs_small.zip'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"Us7PGHMwNv8V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619615778111,"user_tz":-540,"elapsed":1901,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"4c6d2278-65fb-402e-e3c9-08f8b98ecf57"},"source":["!mkdir data"],"execution_count":21,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘data’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bC_Fm0pZNv8W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619615786858,"user_tz":-540,"elapsed":9710,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"f275e26d-844a-4794-eb54-94ae7e7746a1"},"source":["## 압축 풀기\n","!unzip -q ./cats_and_dogs_small.zip -d data/cats_and_dogs_small"],"execution_count":22,"outputs":[{"output_type":"stream","text":["replace data/cats_and_dogs_small/test/cats/cat.1500.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wVHkLprjOEtw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFaaZGzmOEd_","executionInfo":{"status":"ok","timestamp":1619615788778,"user_tz":-540,"elapsed":397,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}}},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","def get_generators():\n","    '''\n","    train, validation, test generator를 생성해서 반환.\n","    train generator는 image 변환 처리\n","    '''\n","    train_dir = './data/cats_and_dogs_small/train'\n","    validation_dir = './data/cats_and_dogs_small/validation'\n","    test_dir = './data/cats_and_dogs_small/test'  # 상대경로라서 맨앞에 . 이있다.\n","    train_datagen = ImageDataGenerator(rescale=1/255,\n","                                       rotation_range=40,\n","                                       brightness_range=(0.7,1.3),\n","                                       zoom_range=0.2,\n","                                       horizontal_flip=True)\n","    test_datagen = ImageDataGenerator(rescale=1/255) #validation/test에서 사용\n","    # generator 들 생성\n","    train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                        target_size=(150,150),\n","                                                        batch_size=N_BATCHS,\n","                                                        class_mode='binary')\n","    val_generator = test_datagen.flow_from_directory(validation_dir,\n","                                                        target_size=(150,150),\n","                                                        batch_size=N_BATCHS,\n","                                                        class_mode='binary')\n","    test_generator = test_datagen.flow_from_directory(test_dir,\n","                                                        target_size=(150,150),\n","                                                        batch_size=N_BATCHS,\n","                                                        class_mode='binary')\n","    return train_generator, val_generator, test_generator\n","\n"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ke4-BjVgNv8W"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"9pKgRKPvNv8X"},"source":["## Transfer learning (전이학습)\n","- 큰 데이터 셋을 이용해 미리 학습된 pre-trained Model의 Weight를 사용하여 현재 하려는 예측 문제에 활용. \n","- ### Convolution base(Feature Extraction 부분)만 활용\n","    - Convolution base는 이미지에 나타나는 일반적인 특성을 파악하기 위한 부분이므로 재사용할 수 있다.\n","    - Classifier 부분은 학습하려는 데이터셋의 class들에 맞게 변경 해야 하므로 재사용할 수 없다.\n","- Pretrained Convlution layer의 활용 \n","    - Feature extraction\n","        - 학습시 학습되지 않고 Feature를 추출하는 역할만 한다.\n","    - Fine tuning\n","        - 학습시 Pretrained Covolution layer도 같이 학습해서 내 데이터셋에 맞춘다."]},{"cell_type":"markdown","metadata":{"id":"qAUzlvYJNv8X"},"source":["## Feature extraction\n","- 기존의 학습된 network에서 fully connected layer를 제외한 나머지 weight를 고정하고 새로운 목적에 맞는 fully connected layer를 추가하여 추가된 weight만 학습하는 방법\n","- `tensorflow.keras.applications` module이 지원하는  image classification models\n","    - (https://www.tensorflow.org/api_docs/python/tf/keras/applications)    \n","![image.png](attachment:image.png)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MAM8Vfu7Nv8X"},"source":["> ### ImageNet\n",">    - 웹상에서 수집한 약 1500만장의 라벨링된 고해상도 이미지로 약 22,000개 카테고리로 구성된다.\n","\n","> ### ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 대회\n",">   - 2010년 부터 2017년 까지 진행된 컴퓨터 비전 경진대회.\n",">   - ImageNet의 이미지중 **1000개 카테고리 약 120만장의 학습용이미지, 5만장의 검증 이미지, 15만장의 테스트 이미지를** 이용해 대회를 진행한다.\n",">   - **2012년** CNN기반 딥러닝 알고리즘인 **AlexNet**이 2위와 큰 차이로 우승하며 이후 딥러닝 알고리즘이 대세가 되었다. 특히 2015년 우승한 ResNet은 0.036의 에러율을 보이며 우승했는데 이는 사람이 에러율이라 알려진 0.05 보다 높은 정확도였다.\n",">   - ILSVRC에서 우승하거나 좋은 성적을 올린 모델들이 컴퓨터 비전분야 발전에 큰 역할을 해왔으며 이후 다양한 딥러닝 모델의 백본(backbone)으로 사용되고 있다.\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"LDigHwUfNv8Y"},"source":["##  VGG16 모델\n","- ImageNet ILSVRC Challenge 2014에서 2등한 모델로 Simonyan and Zisserman(Oxford Univ.)에 의해 제안\n","    - VGGNet이 준우승을 하긴 했지만, 구조의 간결함과 사용의 편이성으로 인해 1등한 GoogLeNet보다 더 각광받았다\n","- 단순한 구조로 지금까지 많이 사용.\n","- 총 16개 layer로 구성됨.\n","- 네트워크 깊이가 어떤 영향을 주는 지 연구 하기 위해 설계된 네트워크로 동일한 kernel size에 convolution의 개수를 늘리는 방식으로 구성됨.\n","    - 11 layer, 13 layer, 16 layer, 19 layer 의 네트워크를 테스트함. \n","    - 19 layer의 성능이 16 layer보다 크게 나아지지 않음\n","- Filter의 수가 64, 128, 256, 512 두 배씩 커짐 \n","- 항상 $3 \\times 3$ filter, Stride=1, same padding, $2\\times 2$ MaxPooling 사용\n","    - 이전 AlexNet이 5 X 5 필터를 사용했는데 VGG16은 3 X 3 필터 두개를 쌓아 사용했다.\n","        - 3 x 3 필터 두개를 쌓는 것이 5 x 5  하나는 사용하는 보다 더 적은 파라미터를 사용하며 성능이 더 좋았다.\n","    - Feature map의 사이즈를 convolution layer가 아닌 Max Pooling 을 사용해 줄여줌.\n","- VGG16의 단점은 마지막에 분류를 위해 Fully Connected Layer 3개를 붙여 파라미터 수가 너무 많아 졌다. 약 1억4천만 개의 parameter(가중치)중 1억 2천만개 정도가 Fully Connected Layer의 파라미터 임.\n","![image-3.png](attachment:image-3.png)"]},{"cell_type":"markdown","metadata":{"id":"OURGPFr5Nv8Y"},"source":["## ResNet (Residual Networks)\n","- 이전 모델들과 비교해 shortcut connection기법을 이용해 Layer수를 획기적으로 늘린 CNN 모델로 ILSVRC 2015년 대회에서 우승을 차지함.\n","\n","![image.png](attachment:image.png)\n","\n","- 레이어를 깊게 쌓으면 성능이 더 좋아 지지 않을까? 실제는 Test 셋 뿐만 아니라 Train Set에서도 성능이 나쁘게 나옴.\n","- Train set에서도 성능이 나쁘게 나온 것은 최적화 문제로 보고, 레이어를 깊게 쌓으면 최적화 하기가 어렵다고 생각함. \n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"HkyeXcjXNv8Z"},"source":["### Idea\n","![image-3.png](attachment:image-3.png)"]},{"cell_type":"markdown","metadata":{"id":"od-nKlvQNv8Z"},"source":["- 입력값을 그대로 출력하는 identity block 을 사용하면 성능이 떨어지지는 않는다.\n","- 그럼 Convloution block을 identity block으로 만들면 최소한 성능은 떨어지지 않고 깊은 Layer를 쌓을 수 있지 않을까?"]},{"cell_type":"markdown","metadata":{"id":"3CN4dH6YNv8Z"},"source":["### Solution\n","- Residual block\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"lyDrbiuBNv8a"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"Y_EDnXTwNv8a"},"source":["- 기존 Layer들의 목표는 입력값인 X를 출력값인 Y로 최적의 매핑할 수 있는 함수 H(X)를 찾는 것이다. 그래서 H(X) – Y 가 최소값이 되는 방향으로 학습을 진행하면서 H(X)를 찾음. 그런데 레이어가 깊어지면서 최적화에 어려움으로 성능이 떨어지는 문제가 발생\n","\n","- ResNet은 layer를 통과해서 나온 값이 **입력값과 동일하게 만드는 것을 목표로 하는 Identity block을** 구성한다.\n","- Identity block은 입력값 X를 레이어를 통과시켜서 나온 Y에 입력값 X를 더해서 합치도록 구성한다.\n","\n","$$\\large H(x) = F(x) + x\\\\x: input,\\;H(x): output,\\;F(x): layer통과값$$ \n"," \n"," \n","- 목표는 $H(x)$(레이어통과한 값) 가 input인 x와 동일한 것이므로 F(x)를 0으로 만들기 위해 학습을 한다. \n","- $F(x)$는 **잔차(Residual)**가 된다. 그리고 잔차인 $F(x)$가 0이 되도록 학습하는 방식이므로 Residual Learning이라고 한다.\n","- 입력인 x를 직접 전달하는 것을 **shortcut connection** 또는 **identity mapping** 또는 **skip connection** 이라고 한다.\n","    - 이 shortcut은 파라미터 없이 단순히 값을 더하는 구조이므로 연산량에 크게 영향이 없다.\n","- 그리고 Residual을 찾는 레이어를 **Residual Block, Identity Block** 이라고 한다.      \n","\n","### 성능향상\n","- $H(x) = F(x) + x$ 을 $x$에 대해 미분하면 최소한 1이므로 Gradient Vanishing 문제를 극복한다.\n","- 잔차학습이라고 하지만 Residual block 은 Convolution Layer와 Activation Layer로 구성되어 있기 때문에 이 Layer를 통과한 Input으로 부터 Feature map을 추출하는 과정은 진행되며 레이어가 깊으므로 다양한 더욱 풍부한 특성들을 추출하게 되어 성능이 향상된다.\n","  "]},{"cell_type":"markdown","metadata":{"id":"DdLck87PNv8a"},"source":["### ResNet 구조\n","- Residual block들을 쌓는 구조\n","    - 일반 Convolution Layer(backbone)을 먼저 쌓고 Identity(Residual) block들을 계속 쌓는다.\n","- 모든 Identity block은 두개의 3X3 conv layer로 구성됨.\n","- 일정 레이어 수별로 filter의 개수를 두배로 증가시키며 stride를 2로 하여 downsampling 함. (Pooling Layer는 Identity block의 시작과 마지막에만 적용)\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"NQGBuJMYNv8b"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"xq4D1cobNv8b"},"source":["## Pretrained Model 사용\n","- tensorflow.keras.applications 패키지를 통해 제공\n","- 모델이름이 클래스이름\n","    - VGG16, ResNet153 등등\n","- 생성자 매개변수\n","    - `weights`: 모형의 학습된 weight. 기본값- 'imagenet'\n","        (imagenet에 개,고양이카테고리가 이미 있음)\n","    - `include_top`: fully connected layer를 포함할지 여부. True 포함시킴, False: 포함 안 시킴\n","    - `input_shape`: 사용자가 입력할 이미지의 크기 shape. 3D 텐서로 지정. (높이, 너비, 채널). 기본값: (224,224, 3)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k47Vx6_-OrVc","executionInfo":{"elapsed":6884,"status":"ok","timestamp":1619606650410,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"bcaf9f08-05d8-4af5-ae70-93513a7ec4f9"},"source":["from tensorflow.keras.applications import VGG16, ResNet50V2\n","\n","conv_base = VGG16(weights= 'imagenet', # imagenet 데이터셋을 \"학습된 가중치(파라미터)\"사용\n","                   #내가 학습한 가중치를 사용하고 싶다면    weights 경로를 넣어 주면 된다. \n","                  include_top = False, # claasigication (Fullyconnected layer)는 가져오지않겠다.\n","                  # true 일때는 모델 전체를 갖고옴(분류classification 레이어까지 갖고옴)\n","                  input_shape =(150,150,3))\n","# conv_base= ResNet50V2(weights= 'imagenet',\n","#                       include_top = False,\n","#                       input_shape=(150,150,3))\n","\n","# 사용하고자 하는 class 이름만 바꾸면.. 매개변수 거의 똑같다."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4_28rhYOrLh","executionInfo":{"elapsed":771,"status":"ok","timestamp":1619606654658,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"247e2dcd-7f6d-418b-9171-563dd30a2e22"},"source":["conv_base.summary()  # fullyconnected 가없다.. /conv2_block1_out (Add)          "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lkGSwgT4I00Q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PmMAQ2HwNv8c"},"source":["## Feature extraction의 두 가지 방법\n","1. **빠른 추출방식**\n","    - 예측하려는 새로운 데이터를 위의 `conv_base`에 입력하여 나온 출력값을 numpy 배열로 저장하고 이를 분류 모델의 입력값으로 사용. Convolution operation을 하지 않아도 되기 때문에 빠르게 학습. 하지만 data augmentation 방법을 사용할 수 없음.\n","\n","2. **받아온 특성 Layer를 이용해 새로운 모델 구현하는 방식**\n","    - 위의 `conv_base` 이후에 새로운 layer를 쌓아 확장한 뒤 전체 모델을 다시 학습. 모든 데이터가 convolution layer들을 통과해야 하기 때문에 학습이 느림. 단 conv_base의 가중치는 업데이트 되지 않도록 한다. data augmentation 방법을 사용할 수 있음."]},{"cell_type":"markdown","metadata":{"id":"6bSrliynNv8d"},"source":["### 빠른 특성 추출 방식\n","\n","-conv_base에서 피처 맵만 추출하고(conv_base는 학습은 안시킨다) , classification layer(Fullyconnect latyer)를 따로 만들어서 학습시킨다~!\n"]},{"cell_type":"markdown","metadata":{"id":"zYfM1dNYNv8d"},"source":["- `conv_base`의 predict 메소드로 입력 이미지의 feature를 추출 "]},{"cell_type":"code","metadata":{"id":"Vp9sNUy0Nv8d","executionInfo":{"status":"ok","timestamp":1619620594649,"user_tz":-540,"elapsed":1106,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}}},"source":["# 하이퍼파라미터\n","LEARNING_RATE = 0.001  \n","N_EPOCHS = 30\n","N_BATCHS = 100"],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffUSjXgDNv8d","executionInfo":{"status":"ok","timestamp":1619620594651,"user_tz":-540,"elapsed":411,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.applications import VGG16 , ResNet50V2\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","\n","np.random.seed(1)\n","tf.random.set_seed(1)"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFy6AT7jNVU_"},"source":["# conv_base 를 통과한 최종 ㄴfeature map size를 확인 할때 이렇게 해봐야함 \n","conv_base = VGG16(weights= 'imagenet',\n","                  include_top = False, input_shape = (150,150,3))\n","\n","conv_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5RO_MooNv8e"},"source":["# 2,3교시  함수 만드는 부분 다시 보기 \n","# featurmap 까지만 추출하는 함수를 만든다. \n","def extract_featuremap(image_directory, sample_counts):\n","    \"\"\"\n","    매개변수로 받은 디렉토리의 이미지들을 \"\"Conv_base(VGG16)\"\" 모델을 통과시켜서 Featuremap들을 추출해서 반환하는 함수.\n","    [매개변수] \n","        image_directory: 이미지 데이터들이 있는 디렉토리\n","        sample_counts: 특성을 추출할 이미지 개수 \n","    [반환값]\n","        튜플 :(featuremap들 , label)\n","\n","        sample_counts : 몇개의 이미지를 추출할지 ..\n","    \"\"\"\n","    conv_base = VGG16(weights= 'imagenet',\n","                      include_top = False, input_shape = (150,150,3))\n","    \n","    # 결과를 담을 ndarray // VGG16 이아닌 모델을 쓸때 (4,4,512) 이부분 바꿔줘야함.맨마지막 레이어의 featuremap 사이즈로 바꿔야함\n","    return_features = np.zeros(shape= (sample_counts, 4,4,512)) # featuremap 저장할 비어있는 배열 (conv_base를 통과한).\n","     # 어떤 모델을 쓰느냐에 따라서 conv_base의 맨마지막의 layer의 output의 shape(=featuremap size)에 맞춘다.\n","    return_labels = np.zeros(shape= (sample_counts,)) #label들저장/1차원으로 만듦..\n","\n","    datagen = ImageDataGenerator(rescale= 1./255) # 이미지를 읽어와야지~\n","    iterator = datagen.flow_from_directory(image_directory,\n","                                           target_size =(150,150),\n","                                           batch_size = N_BATCHS,\n","                                           class_mode = 'binary')\n","    # datagen.folow_from_directory 니까 이미지와 라벨을 튜플로 묶어서 batch_size 만큼 이터레이터를 반환하는것이다..\n","\n","\n","    i = 0 # 반복횟수를 저장할 변수.\n","\n","    for input_batch, label_batch in iterator: #(image, label)* batch 크기(100)\n","        # input_batch를 conv_base 넣어서 featuremap 을 추출. \n","        #모델.predict() - 모델의 레이어들 통과해서 나온 출력결과를 반환.\n","        fm = conv_base.predict(input_batch)\n","        # 슬라이싱 해서 batch 사이즈만큼씩 conv_base를 지난 이미지를 넣어준다.\n","        return_features[i*N_BATCHS:(i+1)*N_BATCHS] = fm\n","        return_labels[i*N_BATCHS : (i+1)*N_BATCHS] = label_batch\n","\n","        i+=1\n","        if i*N_BATCHS >= sample_counts: # 결과를 저장할 배열의 시작 index가 sample_counts보다 크면 반복문 멈추기 \n","            break\n","    return return_features, return_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"cMlrmI4uNv8e","outputId":"b9b9e58e-8803-42cb-a6ed-8b31f7c9ee64"},"source":["train_dir = '/content/data/cats_and_dogs_small/train'\n","validation_dir = '/content/data/cats_and_dogs_small/validation'\n","test_dir ='/content/data/cats_and_dogs_small/test'\n","\n","#Featuremap 추출\n","\n","train_features, train_label = extract_featuremap(train_dir, 2000)\n","validation_features, validation_label = extract_featuremap(validation_dir,1000)\n","test_features, test_label = extract_featuremap(test_dir, 1000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4Rn58JLNv8e","executionInfo":{"elapsed":975,"status":"ok","timestamp":1619577623228,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"8fe5fa4d-d4d0-4ba0-f123-e60cbd890de2"},"source":["train_features.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2000, 4, 4, 512)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"TnSsxgcRemBE"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0698yWkNv8e","executionInfo":{"elapsed":700,"status":"ok","timestamp":1619577629211,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"3288d5c8-b507-4477-d7b9-c5d4e9e3ae1f"},"source":["train_label.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2000,)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"VdPrPuuJNv8f"},"source":["def create_model():\n","    #분류기 모델만 생성\n","    model = keras.Sequential()\n","    model.add(layers.Input((4,4,512)))\n","    model.add(layers.GlobalAveragePooling2D())\n","    model.add(layers.Dense(256, activation= 'relu'))\n","    model.add(layers.Dense(1, activation= 'sigmoid'))\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c31nvPzKNv8f","executionInfo":{"elapsed":560,"status":"ok","timestamp":1619577839736,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"4086c996-d807-45a6-b849-3063ca1ba023"},"source":["model = create_model()\n","\n","model.compile(optimizer= keras.optimizers.Adam(learning_rate= LEARNING_RATE),\n","              loss = 'binary_crossentropy',\n","              metrics = ['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","global_average_pooling2d (Gl (None, 512)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               131328    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 131,585\n","Trainable params: 131,585\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6BE5zeXXLhV","executionInfo":{"elapsed":5498,"status":"ok","timestamp":1619578098431,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"7cd50eef-0864-47b2-e08f-ea5a37125a55"},"source":["history = model.fit(train_features, train_label, #conv_base 를 지나온 넘파이배열로 저장된 feature map들..\n","                    epochs = N_EPOCHS,\n","                    validation_data = (validation_features,validation_label), # 데이터 셋을 안만들어서 그냥 튜플로 넣어줌.\n","                    batch_size = N_BATCHS)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","20/20 [==============================] - 1s 15ms/step - loss: 0.6257 - accuracy: 0.6221 - val_loss: 0.4309 - val_accuracy: 0.8290\n","Epoch 2/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4124 - accuracy: 0.8266 - val_loss: 0.3547 - val_accuracy: 0.8480\n","Epoch 3/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.3406 - accuracy: 0.8547 - val_loss: 0.3374 - val_accuracy: 0.8410\n","Epoch 4/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.3055 - accuracy: 0.8692 - val_loss: 0.2835 - val_accuracy: 0.8840\n","Epoch 5/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2869 - accuracy: 0.8821 - val_loss: 0.2707 - val_accuracy: 0.8870\n","Epoch 6/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2470 - accuracy: 0.8983 - val_loss: 0.2684 - val_accuracy: 0.8910\n","Epoch 7/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2444 - accuracy: 0.8955 - val_loss: 0.2781 - val_accuracy: 0.8830\n","Epoch 8/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2354 - accuracy: 0.8996 - val_loss: 0.2817 - val_accuracy: 0.8790\n","Epoch 9/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.2314 - accuracy: 0.9140 - val_loss: 0.2524 - val_accuracy: 0.8930\n","Epoch 10/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.2126 - accuracy: 0.9057 - val_loss: 0.2586 - val_accuracy: 0.8930\n","Epoch 11/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1927 - accuracy: 0.9201 - val_loss: 0.2495 - val_accuracy: 0.8920\n","Epoch 12/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.2043 - accuracy: 0.9211 - val_loss: 0.2718 - val_accuracy: 0.8840\n","Epoch 13/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1996 - accuracy: 0.9189 - val_loss: 0.2559 - val_accuracy: 0.8940\n","Epoch 14/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1857 - accuracy: 0.9217 - val_loss: 0.2581 - val_accuracy: 0.8920\n","Epoch 15/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1790 - accuracy: 0.9291 - val_loss: 0.2552 - val_accuracy: 0.8940\n","Epoch 16/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1929 - accuracy: 0.9206 - val_loss: 0.2584 - val_accuracy: 0.8930\n","Epoch 17/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1790 - accuracy: 0.9343 - val_loss: 0.2614 - val_accuracy: 0.8940\n","Epoch 18/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1635 - accuracy: 0.9478 - val_loss: 0.2574 - val_accuracy: 0.8950\n","Epoch 19/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1571 - accuracy: 0.9425 - val_loss: 0.2571 - val_accuracy: 0.8910\n","Epoch 20/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1544 - accuracy: 0.9411 - val_loss: 0.2666 - val_accuracy: 0.8920\n","Epoch 21/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1546 - accuracy: 0.9371 - val_loss: 0.2617 - val_accuracy: 0.8900\n","Epoch 22/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1470 - accuracy: 0.9497 - val_loss: 0.2641 - val_accuracy: 0.8910\n","Epoch 23/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1432 - accuracy: 0.9456 - val_loss: 0.2796 - val_accuracy: 0.8840\n","Epoch 24/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1390 - accuracy: 0.9532 - val_loss: 0.3023 - val_accuracy: 0.8820\n","Epoch 25/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1503 - accuracy: 0.9357 - val_loss: 0.2807 - val_accuracy: 0.8900\n","Epoch 26/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1248 - accuracy: 0.9603 - val_loss: 0.2892 - val_accuracy: 0.8840\n","Epoch 27/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1290 - accuracy: 0.9453 - val_loss: 0.2930 - val_accuracy: 0.8830\n","Epoch 28/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1220 - accuracy: 0.9544 - val_loss: 0.2735 - val_accuracy: 0.8940\n","Epoch 29/30\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1188 - accuracy: 0.9585 - val_loss: 0.2793 - val_accuracy: 0.8930\n","Epoch 30/30\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1197 - accuracy: 0.9551 - val_loss: 0.2860 - val_accuracy: 0.8890\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oLTx1wKnXLdx"},"source":["# 한개 이미지 추론 \n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","def predict_cat_dog(path, model, mode = False):\n","    class_name = ['고양이', '강아지']\n","    img = load_img(path, target_size = (150,150,3))\n","    sample = img_to_array(img)[np.newaxis,...]\n","    sample = sample/255.\n","    # 만든 모델이 받는 input이 featuremap 이기 때문에 convloution을  거쳐와야한다..\n","    if mode: # conv_base를 거치도록\n","        cb = VGG16(include_top= False, weights = 'imagenet', input_shape=(150,150,3))\n","        sample = cb.predict(sample)\n","\n","    pred = model.predict(sample)\n","    pred_class = np.where(pred<0.5, 0 , 1)\n","    pred_class_name = class_name[pred_class[0,0]]\n","    return pred, pred_class, pred_class_name\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IEJ9yUN0XLbJ","executionInfo":{"elapsed":1361,"status":"ok","timestamp":1619578919714,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"c96428b4-c963-4928-9082-0ae2c0aa06b8"},"source":["predict_cat_dog('/content/dog.jpg', model, mode = True)  # conv 를 지나치게 하려고 한다."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[0.95421654]], dtype=float32), array([[1]]), '강아지')"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztHFGoTuXLRZ","executionInfo":{"elapsed":1208,"status":"ok","timestamp":1619578938298,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"3f9bf3f0-75bc-4f68-da3a-b42f34b9edda"},"source":["predict_cat_dog('/content/cat.jpg', model, mode = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 44 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa83c2c5050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(array([[0.02184065]], dtype=float32), array([[0]]), '고양이')"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"kxmleqXQbch8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-OWe4gSvNv8f"},"source":["### Pretrained Network를 이용해 새로운 모델 구현하는 방식"]},{"cell_type":"markdown","metadata":{"id":"1Vjw3dONNv8f"},"source":["- Conv_base의 feature extraction 부분에 fully connected layer를 추가하여 모형 생성 \n","- Conv_base에서 가져온 부분은 학습을 하지 않고 weight를 고정\n","    -  **Layer.trainable=False**"]},{"cell_type":"code","metadata":{"id":"QcOH4sFdNv8g"},"source":["LEARNING_RATE = 0.001\n","N_EPOCHS = 20\n","N_BATCHS = 100\n","\n","IMAGE_SIZE = 150"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D87xOKnWNv8g"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","def create_model():\n","    conv_base = VGG16(weights= 'imagenet',\n","                      include_top = False,\n","                      input_shape =(IMAGE_SIZE,IMAGE_SIZE,3))\n","    \n","    conv_base.trainable = False # 학습시 weight 최적화(update)를 하지 않도록 설정.\n","    # 사용할때는 모델 compile 전에 실행해야함.. \n","    # 순전파 할때만 쓰이고 역전파 할때는 업데이트가 되지 않는다..(여기서는 VGG16 이 )\n","\n","    \n","    model = keras.Sequential()\n","    model.add(conv_base) # 한번에 convbase 구조가 다 들어온것임.웨이트도 학습된게 붙음\n","    # // 위에거 summary 해서 다시 공부~!\n","\n","    model.add(layers.GlobalAveragePooling2D()) # Flatten 대신 사용\n","    model.add(layers.Dense(256, activation ='relu'))\n","\n","    # 출력\n","    model.add(layers.Dense(1, activation ='sigmoid'))\n","\n","    return model\n","\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4r2lZE_Nv8g","executionInfo":{"elapsed":1007,"status":"ok","timestamp":1619510290198,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"e57babf1-9f2f-428d-c16f-5c8ae78e2556"},"source":["model = create_model()\n","model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","              loss = 'binary_crossentropy', metrics =['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 512)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               131328    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 14,846,273\n","Trainable params: 131,585\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCtB7OYNNv8g","executionInfo":{"elapsed":883,"status":"ok","timestamp":1619510531321,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"1a7137be-97df-47b5-ead2-76d46023587d"},"source":["train_iterator, validation_iterator, test_iterator = get_generators()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8SomCwMNv8g","executionInfo":{"elapsed":429985,"status":"ok","timestamp":1619511045663,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"},"user_tz":-540},"outputId":"654fe4a4-8ebd-4a5d-fc6f-14f8ef6fe39a"},"source":["history = model.fit(train_iterator,\n","                    epochs = N_EPOCHS,\n","                    steps_per_epoch = len(train_iterator),\n","                    validation_data = validation_iterator,\n","                    validation_steps = len(validation_iterator))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","20/20 [==============================] - 59s 976ms/step - loss: 0.6604 - accuracy: 0.5981 - val_loss: 0.4737 - val_accuracy: 0.8030\n","Epoch 2/20\n","20/20 [==============================] - 20s 984ms/step - loss: 0.4886 - accuracy: 0.7944 - val_loss: 0.3737 - val_accuracy: 0.8510\n","Epoch 3/20\n","20/20 [==============================] - 20s 980ms/step - loss: 0.4123 - accuracy: 0.8157 - val_loss: 0.3289 - val_accuracy: 0.8670\n","Epoch 4/20\n","20/20 [==============================] - 19s 964ms/step - loss: 0.3802 - accuracy: 0.8334 - val_loss: 0.3057 - val_accuracy: 0.8780\n","Epoch 5/20\n","20/20 [==============================] - 19s 970ms/step - loss: 0.3703 - accuracy: 0.8357 - val_loss: 0.2968 - val_accuracy: 0.8880\n","Epoch 6/20\n","20/20 [==============================] - 19s 970ms/step - loss: 0.3460 - accuracy: 0.8545 - val_loss: 0.3082 - val_accuracy: 0.8690\n","Epoch 7/20\n","20/20 [==============================] - 19s 970ms/step - loss: 0.3395 - accuracy: 0.8475 - val_loss: 0.2805 - val_accuracy: 0.8900\n","Epoch 8/20\n","20/20 [==============================] - 19s 973ms/step - loss: 0.3291 - accuracy: 0.8604 - val_loss: 0.2744 - val_accuracy: 0.8910\n","Epoch 9/20\n","20/20 [==============================] - 20s 980ms/step - loss: 0.3267 - accuracy: 0.8562 - val_loss: 0.2765 - val_accuracy: 0.8850\n","Epoch 10/20\n","20/20 [==============================] - 19s 971ms/step - loss: 0.3449 - accuracy: 0.8432 - val_loss: 0.2645 - val_accuracy: 0.8890\n","Epoch 11/20\n","20/20 [==============================] - 19s 962ms/step - loss: 0.3209 - accuracy: 0.8606 - val_loss: 0.2645 - val_accuracy: 0.8910\n","Epoch 12/20\n","20/20 [==============================] - 19s 968ms/step - loss: 0.3273 - accuracy: 0.8545 - val_loss: 0.2603 - val_accuracy: 0.8900\n","Epoch 13/20\n","20/20 [==============================] - 19s 974ms/step - loss: 0.3209 - accuracy: 0.8555 - val_loss: 0.2602 - val_accuracy: 0.8940\n","Epoch 14/20\n","20/20 [==============================] - 19s 969ms/step - loss: 0.3066 - accuracy: 0.8652 - val_loss: 0.2592 - val_accuracy: 0.8960\n","Epoch 15/20\n","20/20 [==============================] - 19s 974ms/step - loss: 0.2699 - accuracy: 0.8901 - val_loss: 0.2612 - val_accuracy: 0.8900\n","Epoch 16/20\n","20/20 [==============================] - 19s 975ms/step - loss: 0.3184 - accuracy: 0.8709 - val_loss: 0.2622 - val_accuracy: 0.8910\n","Epoch 17/20\n","20/20 [==============================] - 19s 970ms/step - loss: 0.2839 - accuracy: 0.8873 - val_loss: 0.2542 - val_accuracy: 0.8940\n","Epoch 18/20\n","20/20 [==============================] - 19s 975ms/step - loss: 0.3141 - accuracy: 0.8468 - val_loss: 0.2541 - val_accuracy: 0.8970\n","Epoch 19/20\n","20/20 [==============================] - 19s 978ms/step - loss: 0.2875 - accuracy: 0.8695 - val_loss: 0.2567 - val_accuracy: 0.8940\n","Epoch 20/20\n","20/20 [==============================] - 19s 975ms/step - loss: 0.2853 - accuracy: 0.8705 - val_loss: 0.2552 - val_accuracy: 0.9000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WuLlCsuhNv8i"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yPZWC67yNv8i"},"source":["## 미세조정(Fine-tuning)\n","- Pretrained 모델을 내가 학습시켜야 하는 데이터셋(Custom Dataset)에 재학습시키는 것을 fine tunning 이라고 한다.\n","- 주어진 문제에 더 적합하도록 모델의 가중치들을 조정."]},{"cell_type":"markdown","metadata":{"id":"u0F60Wm3Nv8i"},"source":["### Fine tuning 전략\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"mfKT4nQgNv8j"},"source":["- **세 전략 모두 classifier layer들은 train한다.**\n","- layer  단위로 trainable을 설정해줘서 조절한다.\n","\n","1. <span style=\"font-size:1.2em;font-weight:bold\">전체 모델을 전부 학습시킨다.(1번)</span>\n","    - Pretrained 모델의 weight는 Feature extraction 의 초기 weight 역할을 한다.\n","    - **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **낮은 경우** 적용.\n","    - 학습에 시간이 많이 걸린다.\n","2. <span style=\"font-size:1.2em;font-weight:bold\">Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습시킨다.(2번)</span>\n","    - **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n","    - **Train dataset의 양이 적고** Pretained 모델이 학습했던 dataset과 custom dataset의 class간의 유사성이 **낮은 경우** 적용\n","3. <span style=\"font-size:1.2em;font-weight:bold\">Pretrained 모델 전체를 고정시키고 classifier layer들만 학습시킨다.(3번)</span>\n","    - **Train dataset의 양이 적고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n","  \n","  \n","> custom dataset: 내가 학습시키고자 하는 dataset "]},{"cell_type":"code","metadata":{"id":"VOoaTMY6Nv8j"},"source":["cb = VGG16(weights= 'imagenet', include_top= False, input_shape=(150,150,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oN-4dEeNv8j","executionInfo":{"status":"ok","timestamp":1619615473889,"user_tz":-540,"elapsed":869,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"805f1c84-e6c2-4b0a-c717-8be94b21b147"},"source":["cb.summary()  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_6 (InputLayer)         [(None, 150, 150, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9CG6FRjNv8k","executionInfo":{"status":"ok","timestamp":1619616576132,"user_tz":-540,"elapsed":632,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"b4c2ea06-d243-4899-9300-0ef561854aaa"},"source":["# network(모델)을 구성하는 layer들을 추출\n","layers = cb.layers # 모델을 구성하는 layer 호출 \n","type(layers), len(layers) # model을 구성하는 layer들을 추출해서 list에 묶어서 반환."],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(list, 19)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybJqxVvfq75t","executionInfo":{"status":"ok","timestamp":1619616584318,"user_tz":-540,"elapsed":624,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"a577b8e4-a907-46e7-8a5f-3c28194e1888"},"source":["layers"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f8698b42b90>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698d87490>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698bc6d10>,\n"," <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f8698bc6bd0>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698b4d550>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698b42690>,\n"," <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f8698bb1250>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698b42110>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8697c3d450>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698b8b850>,\n"," <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f8697c47790>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698c636d0>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698baba10>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698bbc990>,\n"," <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f8698c5cfd0>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698d36110>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698bad6d0>,\n"," <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698b67ad0>,\n"," <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f8698cf5bd0>]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvmMiCdx9MYR","executionInfo":{"status":"ok","timestamp":1619617742631,"user_tz":-540,"elapsed":563,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"71570250-2aa0-4047-bc89-495e67dd778c"},"source":["layers[2]  # 2번인덱스의 layer / layers[2].trainable = False -> 2번 레이어 train 안함."],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698bc6d10>"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5kl8xjml9MVK","executionInfo":{"status":"ok","timestamp":1619617745202,"user_tz":-540,"elapsed":1520,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"58c85ddd-39ca-4c6a-d909-859848ff5ed7"},"source":["# 레이어의 이름 \n","layers[2].name  # 슬라이싱으로 했을때는 에러남.."],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'block1_conv2'"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkY_W7xCugu7","executionInfo":{"status":"ok","timestamp":1619617760591,"user_tz":-540,"elapsed":1038,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"65b9a1e0-4715-4379-fc08-8f8a006bc159"},"source":["l = cb.get_layer('block1_conv2')\n","l"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698bc6d10>"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olq_TIZ09MTI","executionInfo":{"status":"ok","timestamp":1619617763244,"user_tz":-540,"elapsed":530,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"491440ca-95f1-4172-d2bf-fcb34fa4d481"},"source":["# 모델.get_layer('layer이름') 지정한 이름의 layer를 반환\n","\n","ㅣ = cb.get_layer('block1_conv2')  # 이름으로 layer를 호출.\n","ㅣ"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f8698bc6d10>"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLBL2ZMf9MQ7","executionInfo":{"status":"ok","timestamp":1619617765002,"user_tz":-540,"elapsed":428,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"84a630f1-17bb-47a1-ac90-7f38059917d5"},"source":["# layer의 가중치(weight)를 조회 // layer객체.weights\n","l_w = l.weights\n","type(l_w), len(l_w)#,l_w # [weights, bias],"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(list, 2)"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuxAvZWw9MJp","executionInfo":{"status":"ok","timestamp":1619617766794,"user_tz":-540,"elapsed":428,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"5cf7ce0a-954e-4e58-d658-3c059fa95c36"},"source":["np.shape(l_w[0]), np.shape(l_w[1]) # 채널이 64개로 나오니까 bias 64가되고 /\n","#  3*3 사이즈에 입력값이 64개의 채널을 갖고있으니까 필터 1개가 64개로 구성 / 필터 64개 만듦"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([3, 3, 64, 64]), TensorShape([64]))"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"rtUP3xHy3Amg","executionInfo":{"status":"ok","timestamp":1619619751905,"user_tz":-540,"elapsed":1118,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}}},"source":["del layers"],"execution_count":63,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9xSUIx8wNv8k"},"source":["### Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습"]},{"cell_type":"markdown","metadata":{"id":"abuLSHbHNv8k"},"source":["- Conv_base에서 가장 Top부분에 있는 레이어에 대해 fine-tuning.\n","    - 앞의 layer들은 비교적 일반적이고 재사용 가능한 feature를 학습\n","    - 너무 많은 parameter를 학습시키면 overfitting의 위험이 있음 (특히 새로운 데이터의 수가 적을 때)"]},{"cell_type":"code","metadata":{"id":"MEhd9gazNv8l"},"source":["#block5_conv2,block5_conv3를 trainabel =True 할거임..// 나머지는 freezing (trainabele =False)\n","# 맨마지막 block3_maxpooling2D는 어차피 학습하는게 없으니까(파라미터가 없으니까) 상관없음.. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KY1mESbgyFPd","executionInfo":{"status":"ok","timestamp":1619621025899,"user_tz":-540,"elapsed":560,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}}},"source":["def create_model():\n","    # VGG16 사용 , block5_conv2,block5_conv3 두개의 convolution layer들을 fine tuning 할것이다.\n","    conv_base = VGG16(weights= 'imagenet',include_top= False, input_shape=(150,150,3))\n","    # trainable 설정\n","    is_trainable = False # 중간에 True로 바꾸기위해 변수처리\n","    for layer in conv_base.layers: # 레이어들의 리스트를 하나씩 갖고옴\n","        if layer.name =='block5_conv2': # layer 이름을 이용했음.. True로 바뀐후 그밑에 있는 애들은 True가 적용되면서 trainable이 된다.\n","            is_trainable = True\n","        layer.trainable = is_trainable   # 일단 전부 False 로 바꾸면서 원하는 지점에서 True로 바꾼다\n","\n","    model = keras.Sequential()\n","    model.add(conv_base)\n","    model.add(keras.layers.GlobalAveragePooling2D())\n","    model.add(keras.layers.Dense(256, activation= 'relu'))\n","    model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n","\n","\n","    return model\n"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMBxc4NKyFJ7","executionInfo":{"status":"ok","timestamp":1619621029463,"user_tz":-540,"elapsed":1326,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"64dd505a-d133-4f0c-b0d8-2766123c18cb"},"source":["model = create_model()\n","model.compile(optimizer = keras.optimizers.Adam(learning_rate= LEARNING_RATE), loss = 'binary_crossentropy', metrics = ['accuracy'])\n","model.summary()"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n","_________________________________________________________________\n","global_average_pooling2d_5 ( (None, 512)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 14,846,273\n","Trainable params: 4,851,201\n","Non-trainable params: 9,995,072\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3n-lbI1XyFET","executionInfo":{"status":"ok","timestamp":1619621213770,"user_tz":-540,"elapsed":1061,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"43f53662-c06e-4b73-b12b-15477cb6229f"},"source":["train_iterator,validation_iterator,test_iterator = get_generators() # 이미지 갖고옴"],"execution_count":81,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"sRQRCsvDyFCR","executionInfo":{"status":"error","timestamp":1619621638474,"user_tz":-540,"elapsed":175626,"user":{"displayName":"김대민","photoUrl":"","userId":"09531914677321474328"}},"outputId":"1f4e3285-f9a9-47dc-bc83-b67b1d87b37f"},"source":["N_EPOCHS = 30 \n","mc_callback = keras.callbacks.ModelCheckpoint('./models/cat_dog_model', monitor= 'val_loss', save_best_only= True)\n","model.fit(train_iterator,epochs= N_EPOCHS,steps_per_epoch= len(train_iterator),\n","          validation_data = validation_iterator, validation_steps = len(validation_iterator),\n","          callbacks = [mc_callback])\n","# fine 튜닝을 통해 학습한 데이터 때문에 정확도나 loss가 좋아졌다."],"execution_count":84,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n"," 6/20 [========>.....................] - ETA: 5:41 - loss: 1.5309 - accuracy: 0.5201"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-0afd9e853f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_iterator,epochs= N_EPOCHS,steps_per_epoch= len(train_iterator),\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           callbacks = [mc_callback])\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"QkvVFOqByE-j"},"source":["best_model = keras.models.load_model('./models/cat_dog_model') # 모델 정장햇고, 경로 지정해주면됨. 4/28 zoom_4강의"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCYTAdyRyE7o"},"source":["#evlauation\n","best_model.evaluate(train_iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESEL0qZXNv8l"},"source":["best_model.evaluate(test_iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPidpoOfNv8l"},"source":["predict_cat_dog('/content/cat.jpg', best_model, mode=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtIqmT0fNv8l"},"source":["predict_cat_dog('/content/dog.jpg', best_model, mode=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJNG4-AaNv8l"},"source":["# 베스트 모델을 계속 쓰고 싶다면 zip 명령어를 사용해서 압축해서 다운받아서 사용하면됨~!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SnTTCJVGNv8m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvjVdFN-Nv8m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-a6HCvHNv8m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFRuJEL0Nv8m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJkInQ5NNv8m"},"source":[""],"execution_count":null,"outputs":[]}]}