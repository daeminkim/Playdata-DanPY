{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.;C:\\\\Users\\\\Playdata\\\\ObjectDetection\\\\TF_oda2\\\\models;C:\\\\Users\\\\Playdata\\\\ObjectDetection\\\\TF_oda2\\\\models\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.environ['PYTHONPATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util, config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_CONFIG_PATH = os.path.join('model', 'pipeline.config')\n",
    "CHECK_POINT_PATH = os.path.join('model','checkpoint', 'ckpt-21')\n",
    "LABEL_MAP_FILE_PATH = os.path.join('labelmap','label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x22094da0548>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipline.config에 맞춰서 추출한 모델을 바탕으로 모델을 생성\n",
    "\n",
    "# pipeline.config를 조회 // 컨피그 읽어옴.. \n",
    "config = config_util.get_configs_from_pipeline_file(PIPELINE_CONFIG_PATH)\n",
    "\n",
    "# pipeline.config의 model설정 정보를 넣어서 모델 생성\n",
    "detection_model = model_builder.build(model_config= config['model'], is_training =False)\n",
    "\n",
    "# detection_model -> 학습시킨 모델\n",
    "# 모델에 학습시킨 checkpoint(weight)를 주입\n",
    "# checkpoint 조회\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model = detection_model)\n",
    "ckpt.restore(CHECK_POINT_PATH).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection을 실행하는 함수\n",
    "\n",
    "# 순전파 처리 함수에 @tf.function decorator를 선언하면 실행 속도가 빨라진다.\n",
    "@tf.function\n",
    "def detect_func(image):\n",
    "    \"\"\"\n",
    "    매개변수로 object detection을 수행할 대상 image(Tensor)를 받아서 detection처리.\n",
    "    1. preprocessing(전처리): resize, normalization 작업\n",
    "    2. detection(inference - 추론)\n",
    "    3. detection결과를 post processing : Non Maximum Suppression\n",
    "    4. post processing한 결과를 반환.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. preprocessing\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    # 2. 추론\n",
    "    predict_dict = detection_model.predict(image, shapes)\n",
    "    # 3. post processing\n",
    "    result =detection_model.postprocess(predict_dict, shapes)\n",
    "\n",
    "    return result # 4교시 수업.. 다시.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'id': 1, 'name': 'one'},\n",
       " 2: {'id': 2, 'name': 'two'},\n",
       " 3: {'id': 3, 'name': 'three'},\n",
       " 4: {'id': 4, 'name': 'four'},\n",
       " 5: {'id': 5, 'name': 'five'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_FILE_PATH)\n",
    "print(type(category_index))\n",
    "category_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맥에서는 터미널에서 export PYTHONPATH=$PYTHONPATH:[path]:[path]/slim 입력하고 주피터키시면 되세요!!\n",
    "# path는 경로  직접 입력해주셔야하구요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹캠으로 부터 이미지를 받아서 추론한 결과를 화면에 보여주기.\n",
    "cap = cv2.VideoCapture(0)\n",
    "# 웹켐 width, height 조회\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-dn5w5exm\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2fc5a49e4308>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# BGR -> RGB (모델이 학습할때 RGB 모드로 학습했기 때문에 같은 형식으로 변환)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mimage_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# 0 번축을 늘린다. Tensor로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.2) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-dn5w5exm\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read() # 한 frame 읽기.\n",
    "    frame = cv2.flip(frame,1) # 좌우 반전\n",
    "    \n",
    "    # BGR -> RGB (모델이 학습할때 RGB 모드로 학습했기 때문에 같은 형식으로 변환)\n",
    "\n",
    "    image_np = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # 0 번축을 늘린다. Tensor로 변환\n",
    "    input_tensor = tf.convert_to_tensor(image_np[np.newaxis,...], dtype = tf.float32)\n",
    "    \n",
    "    #추론\n",
    "    post_detection = detect_func(input_tensor) #전처리 -> 추론 -> 후처리\n",
    "    \n",
    "    num_detections = int(post_detection.pop('num_detections'))\n",
    "    # 추론한 결과들을 num_detection 개수 (detection한 물체의 개수) 만큼의 값만 남긴다.\n",
    "    detections ={key:value[0, :num_detections].numpy()   for key, value in post_detection.items()}\n",
    "\n",
    "    # 새로 구성한 결과 dictionary(detections)에 num_detections 값을 추가.\n",
    "    detections['num_detections'] = num_detections\n",
    "    # detection_classes는 검출한 box의 class 값을 label encording 된 값으로 가진다. float32로 반환되는 것dmf int로 변환 처리\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    \n",
    "    \n",
    "    MIN_CONF_THRESH = 0.7 # 물체가 있을 Confidence score가 0.5이상인 bounding box만 나오도록 하겠다.\n",
    "    image_np_with_detection = image_np.copy() # detection한 원본 이미지의 카피본을 생성.\n",
    "    img = viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np_with_detection, # 추론한 원본이미지\n",
    "                    detections['detection_boxes'],  # bounding box 좌표\n",
    "                    detections['detection_classes'] + 1, # bounding box내의 물체 index (class 확률에서 0은 첫번째 label,labelmap의 id는 1 부터 시작하기 때문에 +1)\n",
    "                    detections['detection_scores'], #bounding box 내에 물체가 있을 확률(confidence score)\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates = True, #bounding box의 좌표들이 normalize 되었는지 여부\n",
    "                    max_boxes_to_draw = 200,\n",
    "                    min_score_thresh = MIN_CONF_THRESH) # Confidence score가 얼마 이상인 bounding box만 나오도록 하겠다.\n",
    "    \n",
    "    \n",
    "    # 결과 image를 RGB ->BGR \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # 화면에 출력\n",
    "    cv2.imshow('frame', img)\n",
    "    if cv2.waitKey(1) >0 : # 아무키나 입력하면 \n",
    "        break\n",
    "    \n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post processing 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('five.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = img[np.newaxis, ...]\n",
    "input_tensor = tf.convert_to_tensor(img, dtype= tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_detection = detect_func(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['detection_boxes', 'detection_scores', 'detection_classes', 'num_detections', 'raw_detection_boxes', 'raw_detection_scores', 'detection_multiclass_scores', 'detection_anchor_indices'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection['num_detections'].numpy() # Tensor -> ndarray\n",
    "#num_detection : 후처리로 최종 결과로 나온 bounding box의 개수. \n",
    "#전체 bounding box에서 confidence score 순으로 내림 차순 한 뒤 NMS를 거쳐서 최종적으로 남은 bounding box의개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection['detection_boxes'].shape\n",
    "# [1, 100, 4] -> [추론한 이미지개수, num_detections, 좌표(x,y,w,h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bbox에 물체가 있을 확률 = confidence score\n",
    "post_detection['detection_scores'].shape\n",
    "#[1, 100] -> [추로한 이미지개수, num_detections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0.585899  , 0.33339408, 0.9933591 , 0.632707  ],\n",
       "       [0.67153436, 0.67636776, 0.9882881 , 0.90353   ],\n",
       "       [0.        , 0.12148029, 1.        , 1.        ],\n",
       "       [0.5910687 , 0.34480077, 0.9933917 , 0.6449523 ],\n",
       "       [0.        , 0.18655479, 0.3199943 , 0.5079384 ],\n",
       "       [0.        , 0.        , 0.3835048 , 0.20605263],\n",
       "       [0.10778086, 0.        , 0.4977411 , 0.32233542],\n",
       "       [0.67153436, 0.67636776, 0.9882881 , 0.90353   ],\n",
       "       [0.5972187 , 0.        , 0.9824689 , 0.1816591 ],\n",
       "       [0.83610964, 0.7273355 , 0.9993197 , 0.9454033 ]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection['detection_boxes'][0, :10]  # 마지막 교시..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.585899  , 0.33339408, 0.9933591 , 0.632707  ], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection['detection_boxes'][0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
       "array([[0.9725988 , 0.6281108 , 0.35057196, 0.2724839 , 0.15958795,\n",
       "        0.14284   , 0.08882222, 0.08877608, 0.07228383, 0.07189092,\n",
       "        0.06729928, 0.0644708 , 0.05527753, 0.0516035 , 0.04836428,\n",
       "        0.04740807, 0.04544836, 0.0445638 , 0.04317635, 0.04245842,\n",
       "        0.04229394, 0.04067498, 0.03857976, 0.03775012, 0.03633496,\n",
       "        0.03505528, 0.03480923, 0.03420192, 0.03180853, 0.03121236,\n",
       "        0.03024733, 0.03023371, 0.02901089, 0.02875397, 0.02818114,\n",
       "        0.02759275, 0.02748793, 0.02691084, 0.02684328, 0.02584034,\n",
       "        0.02540469, 0.02519199, 0.02516478, 0.02473003, 0.02425498,\n",
       "        0.02398628, 0.02325255, 0.02275136, 0.02270487, 0.02239472,\n",
       "        0.02202275, 0.02178961, 0.02140054, 0.0210278 , 0.02087599,\n",
       "        0.02059463, 0.02048776, 0.0202468 , 0.01960331, 0.01910436,\n",
       "        0.01900312, 0.0189757 , 0.01892054, 0.01891473, 0.01875782,\n",
       "        0.01854795, 0.01837516, 0.01769209, 0.01762491, 0.01759472,\n",
       "        0.01725018, 0.01688838, 0.0168204 , 0.01681122, 0.01677957,\n",
       "        0.01673079, 0.01672953, 0.01668444, 0.01661181, 0.01645285,\n",
       "        0.0163641 , 0.01619208, 0.01618281, 0.01617479, 0.01608402,\n",
       "        0.0156984 , 0.01569191, 0.01567462, 0.01557735, 0.01555523,\n",
       "        0.01552066, 0.01550907, 0.01542363, 0.01514819, 0.01510307,\n",
       "        0.01508144, 0.01508027, 0.01506147, 0.01504889, 0.01482323]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection['detection_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100, 6])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 class 별 확률\n",
    "post_detection['detection_multiclass_scores'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=\n",
       "array([3.3004880e-03, 4.2986274e-03, 6.1255395e-03, 1.9497991e-02,\n",
       "       7.2723627e-04, 9.7259879e-01], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection['detection_multiclass_scores'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
